{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gradio torch jupyter ipywidgets transformers ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "ollama.pull('mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://localhost:11434')\n",
    "response = client.chat(model='mistral', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='mistral',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.show('mistral:7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "def mistralChat(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.bfloat16)\n",
    "    model = model.to('cuda')\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7, \n",
    "                            return_dict_in_generate=True,do_sample=True)\n",
    "    \n",
    "    tokens = outputs.sequences[0, input_length:]\n",
    "    return tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "Running on public URL: https://f7bb4744bcff4e0717.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f7bb4744bcff4e0717.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.38s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1741, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1296, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/gradio/utils.py\", line 751, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_14697/1274065568.py\", line 9, in mistralChat\n",
      "    model = model.to('cuda')\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2576, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1152, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 802, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 825, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1150, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 90.00 MiB is free. Process 12458 has 658.00 MiB memory in use. Including non-PyTorch memory, this process has 22.91 GiB memory in use. Of the allocated memory 22.05 GiB is allocated by PyTorch, and 410.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as server:\n",
    "    with gr.Tab(\"LLM Inferencing\"):\n",
    "    \n",
    "        model_input = gr.Textbox(label=\"Your Question:\", \n",
    "                                value=\"What’s your question?\", interactive=True)\n",
    "        ask_button = gr.Button(\"Ask\")\n",
    "        model_output = gr.Textbox(label=\"The Answer:\", interactive=False, \n",
    "                                value=\"Answer goes here...\")\n",
    "    \n",
    "    ask_button.click(mistralChat, inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "server.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
