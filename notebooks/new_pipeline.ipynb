{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "# importing required modules \n",
    "from pypdf import PdfReader \n",
    "\n",
    "# creating a pdf reader object \n",
    "reader = PdfReader('../embed_docs/Numenera Discovery.pdf') \n",
    "\n",
    "# printing number of pages in pdf file \n",
    "print(len(reader.pages)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting a specific page from the pdf file \n",
    "page = reader.pages[52] \n",
    "\n",
    "# extracting text from page \n",
    "text = page.extract_text() \n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1597961\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader \n",
    "import nltk\n",
    "\n",
    "file = '../embed_docs/Numenera Discovery.pdf'\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF and split it into sentences\n",
    "text = extract_text_from_pdf(file)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = text[31015:33037]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16731\n"
     ]
    }
   ],
   "source": [
    "# Splitting Text into Sentences\n",
    "def split_text_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "sentences = split_text_into_sentences(text)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us text, sure, but what do we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mistral:7b'\n",
      "'mistral:latest'\n",
      "'mxbai-embed-large:v1'\n",
      "'phi:2.7b'\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "model_list = ollama.list()\n",
    "for model in model_list['models']:\n",
    "    model_name = model['model']  # Access the 'model' attribute directly from each model\n",
    "    pprint(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a summarization with the doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules \n",
    "from pypdf import PdfReader \n",
    "\n",
    "# creating a pdf reader object \n",
    "reader = PdfReader('../embed_docs/Player_s Handbook.pdf') \n",
    "pages = reader.pages\n",
    "page = pages[42]\n",
    "print(page.extract_text(extraction_mode=\"layout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "from typing import List \n",
    "\n",
    "def summarize_from_pages(pages) -> List:\n",
    "    task = \"Summarize this in one to three sentences.\"\n",
    "    response = []\n",
    "    for page in pages:\n",
    "        page_text = page.extract_text(extraction_mode=\"layout\")\n",
    "        task_response = ollama.generate(\n",
    "            model='mistral:7b',\n",
    "            prompt=f\"Using the following context: {page_text}, perform this task: {task}\"\n",
    "        )\n",
    "        response.append(task_response)\n",
    "    return response\n",
    "\n",
    "task_response = summarize_from_pages(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(task_response[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in task_response:\n",
    "    pprint(task['response'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text_chunker in /home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: tqdm in /home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install text_chunker tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "from tqdm import tqdm\n",
    "\n",
    "file = '../embed_docs/Player_s Handbook.pdf'\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF and split it into sentences\n",
    "text = extract_text_from_pdf(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 30000000\n",
    "\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    vecs = np.stack([sent.vector / sent.vector_norm for sent in sents])\n",
    "\n",
    "    return sents, vecs\n",
    "\n",
    "def cluster_text(sents, vecs, threshold):\n",
    "    clusters = [[0]]\n",
    "    for i in range(1, len(sents)):\n",
    "        if np.dot(vecs[i], vecs[i-1]) < threshold:\n",
    "            clusters.append([])\n",
    "        clusters[-1].append(i)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def clean_text(text):\n",
    "    # Add your text cleaning process here\n",
    "    return text\n",
    "\n",
    "# Initialize the clusters lengths list and final texts list\n",
    "clusters_lens = []\n",
    "final_texts = []\n",
    "\n",
    "# Process the chunk\n",
    "threshold = 0.3\n",
    "sents, vecs = process(text)\n",
    "\n",
    "# Cluster the sentences\n",
    "clusters = cluster_text(sents, vecs, threshold)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_txt = clean_text(' '.join([sents[i].text for i in cluster]))\n",
    "    cluster_len = len(cluster_txt)\n",
    "    \n",
    "    # Check if the cluster is too short\n",
    "    if cluster_len < 60:\n",
    "        continue\n",
    "    \n",
    "    # Check if the cluster is too long\n",
    "    elif cluster_len > 3000:\n",
    "        threshold = 0.6\n",
    "        sents_div, vecs_div = process(cluster_txt)\n",
    "        reclusters = cluster_text(sents_div, vecs_div, threshold)\n",
    "        \n",
    "        for subcluster in reclusters:\n",
    "            div_txt = clean_text(' '.join([sents_div[i].text for i in subcluster]))\n",
    "            div_len = len(div_txt)\n",
    "            \n",
    "            if div_len < 60 or div_len > 3000:\n",
    "                continue\n",
    "            \n",
    "            clusters_lens.append(div_len)\n",
    "            final_texts.append(div_txt)\n",
    "            \n",
    "    else:\n",
    "        clusters_lens.append(cluster_len)\n",
    "        final_texts.append(cluster_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "from typing import List \n",
    "\n",
    "def summarize_from_chunks(chunks) -> List:\n",
    "    task = \"Summarize this in one to three sentences.\"\n",
    "    response = []\n",
    "    for chunk in enumerate(chunks):\n",
    "        task_response = ollama.generate(\n",
    "            model='mistral:7b',\n",
    "            prompt=f\"Using the following context: {chunk}, perform this task: {task}\"\n",
    "        )\n",
    "        response.append(task_response)\n",
    "    return response\n",
    "\n",
    "task_response = summarize_from_chunks(final_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3988it [51:13,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "from typing import List \n",
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_and_summarize(chunks) -> List:\n",
    "    task = \"Summarize this in one to three sentences.\"\n",
    "    summaries_and_chunks = []\n",
    "    for chunk in tqdm(enumerate(chunks)):\n",
    "        summary = ollama.generate(\n",
    "            model='mistral:7b',\n",
    "            prompt=f\"Using the following context: {chunk}, perform this task: {task}\"\n",
    "        )\n",
    "        summary_response = summary['response']\n",
    "        summaries_and_chunks.append({'summary': summary_response, 'chunk': chunk})\n",
    "    return summaries_and_chunks\n",
    "\n",
    "s_and_c = chunk_and_summarize(final_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(s_and_c[350])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
