{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "# importing required modules \n",
    "from pypdf import PdfReader \n",
    "\n",
    "# creating a pdf reader object \n",
    "reader = PdfReader('../embed_docs/Numenera Discovery.pdf') \n",
    "\n",
    "# printing number of pages in pdf file \n",
    "print(len(reader.pages)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting a specific page from the pdf file \n",
    "page = reader.pages[52] \n",
    "\n",
    "# extracting text from page \n",
    "text = page.extract_text() \n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1597961\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader \n",
    "import nltk\n",
    "\n",
    "file = '../embed_docs/Numenera Discovery.pdf'\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF and split it into sentences\n",
    "text = extract_text_from_pdf(file)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = text[31015:33037]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16731\n"
     ]
    }
   ],
   "source": [
    "# Splitting Text into Sentences\n",
    "def split_text_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "sentences = split_text_into_sentences(text)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us text, sure, but what do we do with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mistral:7b'\n",
      "'mistral:latest'\n",
      "'mxbai-embed-large:v1'\n",
      "'phi:2.7b'\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "model_list = ollama.list()\n",
    "for model in model_list['models']:\n",
    "    model_name = model['model']  # Access the 'model' attribute directly from each model\n",
    "    pprint(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a summarization with the doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules \n",
    "from pypdf import PdfReader \n",
    "\n",
    "# creating a pdf reader object \n",
    "reader = PdfReader('../embed_docs/Player_s Handbook.pdf') \n",
    "pages = reader.pages\n",
    "page = pages[42]\n",
    "print(page.extract_text(extraction_mode=\"layout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "from typing import List \n",
    "\n",
    "def summarize_from_pages(pages) -> List:\n",
    "    task = \"Summarize this in one to three sentences.\"\n",
    "    response = []\n",
    "    for page in pages:\n",
    "        page_text = page.extract_text(extraction_mode=\"layout\")\n",
    "        task_response = ollama.generate(\n",
    "            model='mistral:7b',\n",
    "            prompt=f\"Using the following context: {page_text}, perform this task: {task}\"\n",
    "        )\n",
    "        response.append(task_response)\n",
    "    return response\n",
    "\n",
    "task_response = summarize_from_pages(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(task_response[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in task_response:\n",
    "    pprint(task['response'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: text_chunker in /home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: tqdm in /home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: numpy in /home/kaminaduck/code/ai-holodeck/venv/lib/python3.10/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install text_chunker tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "from tqdm import tqdm\n",
    "\n",
    "file = '../embed_docs/Player_s Handbook.pdf'\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "    return text\n",
    "\n",
    "# Extract text from the PDF and split it into sentences\n",
    "text = extract_text_from_pdf(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 30000000\n",
    "\n",
    "\n",
    "def process(text):\n",
    "    doc = nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    vecs = np.stack([sent.vector / sent.vector_norm for sent in sents])\n",
    "\n",
    "    return sents, vecs\n",
    "\n",
    "def cluster_text(sents, vecs, threshold):\n",
    "    clusters = [[0]]\n",
    "    for i in range(1, len(sents)):\n",
    "        if np.dot(vecs[i], vecs[i-1]) < threshold:\n",
    "            clusters.append([])\n",
    "        clusters[-1].append(i)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def clean_text(text):\n",
    "    # Add your text cleaning process here\n",
    "    return text\n",
    "\n",
    "# Initialize the clusters lengths list and final texts list\n",
    "clusters_lens = []\n",
    "final_texts = []\n",
    "\n",
    "# Process the chunk\n",
    "threshold = 0.3\n",
    "sents, vecs = process(text)\n",
    "\n",
    "# Cluster the sentences\n",
    "clusters = cluster_text(sents, vecs, threshold)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_txt = clean_text(' '.join([sents[i].text for i in cluster]))\n",
    "    cluster_len = len(cluster_txt)\n",
    "    \n",
    "    # Check if the cluster is too short\n",
    "    if cluster_len < 60:\n",
    "        continue\n",
    "    \n",
    "    # Check if the cluster is too long\n",
    "    elif cluster_len > 3000:\n",
    "        threshold = 0.6\n",
    "        sents_div, vecs_div = process(cluster_txt)\n",
    "        reclusters = cluster_text(sents_div, vecs_div, threshold)\n",
    "        \n",
    "        for subcluster in reclusters:\n",
    "            div_txt = clean_text(' '.join([sents_div[i].text for i in subcluster]))\n",
    "            div_len = len(div_txt)\n",
    "            \n",
    "            if div_len < 60 or div_len > 3000:\n",
    "                continue\n",
    "            \n",
    "            clusters_lens.append(div_len)\n",
    "            final_texts.append(div_txt)\n",
    "            \n",
    "    else:\n",
    "        clusters_lens.append(cluster_len)\n",
    "        final_texts.append(cluster_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "from typing import List \n",
    "\n",
    "def summarize_from_chunks(chunks) -> List:\n",
    "    task = \"Summarize this in one to three sentences.\"\n",
    "    response = []\n",
    "    for chunk in enumerate(chunks):\n",
    "        task_response = ollama.generate(\n",
    "            model='mistral:7b',\n",
    "            prompt=f\"Using the following context: {chunk}, perform this task: {task}\"\n",
    "        )\n",
    "        response.append(task_response)\n",
    "    return response\n",
    "\n",
    "task_response = summarize_from_chunks(final_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3988it [51:10,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "from typing import List \n",
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_and_summarize(chunks) -> List:\n",
    "    task = \"Summarize this in one to three sentences.\"\n",
    "    summaries_and_chunks = []\n",
    "    for chunk in tqdm(enumerate(chunks)):\n",
    "        summary = ollama.generate(\n",
    "            model='mistral:7b',\n",
    "            prompt=f\"Using the following context: {chunk}, perform this task: {task}\"\n",
    "        )\n",
    "        summary_response = summary['response']\n",
    "        summaries_and_chunks.append({'summary': summary_response, 'chunk': chunk})\n",
    "    return summaries_and_chunks\n",
    "\n",
    "s_and_c = chunk_and_summarize(final_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume ollama is already imported and configured\n",
    "def get_embedding(text):\n",
    "    response = ollama.embeddings(\n",
    "        model='mxbai-embed-large:v1',\n",
    "        prompt=text\n",
    "    )\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "def process_summary_items(data):\n",
    "    embeddings = []\n",
    "    for item in data:\n",
    "        # Extracting text from the 'chunk' which is the second element of the tuple\n",
    "        chunk_text = item['chunk'][1]\n",
    "        summary_text = item['summary']\n",
    "\n",
    "        # Generate embeddings\n",
    "        chunk_embedding = get_embedding(chunk_text)\n",
    "        summary_embedding = get_embedding(summary_text)\n",
    "\n",
    "        # Store embeddings in a tuple or a dictionary as needed\n",
    "        embeddings.append({\n",
    "            'chunk_embedding': chunk_embedding,\n",
    "            'summary_embedding': summary_embedding\n",
    "        })\n",
    "    return embeddings\n",
    "\n",
    "# Assuming s_and_c is your list of items\n",
    "all_embeddings = process_summary_items(s_and_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.collections import Collection\n",
    "from weaviate.client import WeaviateClient\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_weaviate_local_client() -> WeaviateClient:\n",
    "    client = weaviate.connect_to_local(\n",
    "        additional_config=wvc.init.AdditionalConfig(\n",
    "            timeout=(60,1800),\n",
    "        ),\n",
    "    )\n",
    "    return client\n",
    "\n",
    "client = create_weaviate_local_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-11 13:16:21.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_collection\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mCreating collection test_collection\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def create_collection(client: WeaviateClient, collection_name: str)-> Collection:\n",
    "    with client: \n",
    "        logger.info(f\"Creating collection {collection_name}\")\n",
    "\n",
    "        client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_transformers(),\n",
    "            vector_index_config=wvc.config.Configure.VectorIndex.hnsw(\n",
    "                distance_metric=wvc.config.VectorDistances.COSINE # select prefered distance metric\n",
    "        ),\n",
    "    )\n",
    "    collection = client.collections.get(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "test_collection = create_collection(client, \"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking data into Weaviate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3988 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m         collection\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39minsert_many(chunk_objs)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks into Weaviate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mload_chunks_into_weaviate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_collection\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 15\u001b[0m, in \u001b[0;36mload_chunks_into_weaviate\u001b[0;34m(chunks, client, collection)\u001b[0m\n\u001b[1;32m     11\u001b[0m chunk_objs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m tqdm(chunks):\n\u001b[1;32m     13\u001b[0m     chunk_obj \u001b[38;5;241m=\u001b[39m wvc\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataObject(\n\u001b[1;32m     14\u001b[0m         properties\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m---> 15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m         },\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Add vector embeddings. Adjust the names if they need to match specific schema in Weaviate\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         vector\u001b[38;5;241m=\u001b[39m[chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m], chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]]  \u001b[38;5;66;03m# Adjust depending on schema\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     )    \n\u001b[1;32m     25\u001b[0m     chunk_objs\u001b[38;5;241m.\u001b[39mappend(chunk_obj)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m client:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.collections import Collection\n",
    "from weaviate.client import WeaviateClient\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_chunks_into_weaviate(chunks: List, client: WeaviateClient, collection: Collection):\n",
    "\n",
    "    print(\"Chunking data into Weaviate\")\n",
    "    chunk_objs = []\n",
    "    for chunk in tqdm(chunks):\n",
    "        chunk_obj = wvc.data.DataObject(\n",
    "            properties={\n",
    "                \"content\": chunk['content'],\n",
    "                \"tokens\": chunk['tokens'],\n",
    "                \"title\": chunk['title'],\n",
    "                \"type\": chunk['type'],\n",
    "                \"url\": chunk['url'],\n",
    "                \"label\": chunk['label']\n",
    "            },\n",
    "            # Add vector embeddings. Adjust the names if they need to match specific schema in Weaviate\n",
    "            vector=[chunk['chunk_embedding'], chunk['summary_embedding']]  # Adjust depending on schema\n",
    "        )    \n",
    "        chunk_objs.append(chunk_obj)\n",
    "        \n",
    "    with client:\n",
    "        collection.data.insert_many(chunk_objs)\n",
    "        \n",
    "    print(f\"Loaded {len(chunks)} chunks into Weaviate\")\n",
    "    \n",
    "load_chunks_into_weaviate(all_embeddings, client, test_collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
