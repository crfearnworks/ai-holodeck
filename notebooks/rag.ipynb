{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using https://learnbybuilding.ai/tutorials/rag-from-scratch as a tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Baby's First RAG \"\"\"\n",
    "\n",
    "corpus_of_documents = [\n",
    "    \"Take a leisurely walk in the park and enjoy the fresh air.\",\n",
    "    \"Visit a local museum and discover something new.\",\n",
    "    \"Attend a live music concert and feel the rhythm.\",\n",
    "    \"Go for a hike and admire the natural scenery.\",\n",
    "    \"Have a picnic with friends and share some laughs.\",\n",
    "    \"Explore a new cuisine by dining at an ethnic restaurant.\",\n",
    "    \"Take a yoga class and stretch your body and mind.\",\n",
    "    \"Join a local sports league and enjoy some friendly competition.\",\n",
    "    \"Attend a workshop or lecture on a topic you're interested in.\",\n",
    "    \"Visit an amusement park and ride the roller coasters.\"\n",
    "]\n",
    "\n",
    "def jaccard_similarity(query, document) -> float:\n",
    "    \"\"\" Pre-processes plain strings into a set to perform comparisons.\n",
    "\n",
    "    Args:\n",
    "        query (_type_): _description_\n",
    "        document (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        float: _description_\n",
    "    \"\"\"\n",
    "    query = query.lower().split(\" \")\n",
    "    document = document.lower().split(\" \")\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def return_response(query, corpus) -> str:\n",
    "    \"\"\" Selects the best document to return to the user\n",
    "\n",
    "    Args:\n",
    "        query (_type_): _description_\n",
    "        corpus (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for doc in corpus:\n",
    "        similarity = jaccard_similarity(user_input, doc)\n",
    "        similarities.append(similarity)\n",
    "    return corpus_of_documents[similarities.index(max(similarities))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What is a leisure activity that you like?\"\n",
    "user_input = \"I like to be with friends\"\n",
    "return_response(user_input, corpus_of_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import ollama\n",
    "ollama.pull('mistral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = \"I like to see my friends\"\n",
    "relevant_document = return_response(user_input, corpus_of_documents)\n",
    "full_response = []\n",
    "# https://github.com/jmorganca/ollama/blob/main/docs/api.md\n",
    "prompt = \"\"\"\n",
    "You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.\n",
    "This is the recommended activity: {relevant_document}\n",
    "The user input is: {user_input}\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\"\n",
    "\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\n",
    "    \"model\": \"mistral:latest\",\n",
    "    \"prompt\": prompt.format(user_input=user_input, relevant_document=relevant_document)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        # count += 1\n",
    "        # if count % 5== 0:\n",
    "        #     print(decoded_line['response']) # print every fifth token\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            if 'response' in decoded_line:\n",
    "                full_response.append(decoded_line['response'])\n",
    "            else:\n",
    "                print(\"Warning: 'response' key not found in the data:\", decoded_line)\n",
    "finally:\n",
    "    response.close()\n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"I don't like to hike\"\n",
    "relevant_document = return_response(user_input, corpus_of_documents)\n",
    "# https://github.com/jmorganca/ollama/blob/main/docs/api.md\n",
    "full_response = []\n",
    "prompt = \"\"\"\n",
    "You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.\n",
    "This is the recommended activity: {relevant_document}\n",
    "The user input is: {user_input}\n",
    "Compile a recommendation to the user based on the recommended activity and the user input.\n",
    "\"\"\"\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\n",
    "    \"model\": \"mistral:latest\",\n",
    "    \"prompt\": prompt.format(user_input=user_input, relevant_document=relevant_document)\n",
    "}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "try:\n",
    "    for line in response.iter_lines():\n",
    "        # filter out keep-alive new lines\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            # print(decoded_line['response'])  # uncomment to results, token by token\n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends the tutorial at https://learnbybuilding.ai/tutorials/rag-from-scratch \n",
    "Now to go into https://learnbybuilding.ai/tutorials/rag-from-scratch-part-2-semantics-and-cosine-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_of_documents = [\n",
    "    \"Take a leisurely walk in the park and enjoy the fresh air.\",\n",
    "    \"Visit a local museum and discover something new.\",\n",
    "    \"Attend a live music concert and feel the rhythm.\",\n",
    "    \"Go for a hike and admire the natural scenery.\",\n",
    "    \"Have a picnic with friends and share some laughs.\",\n",
    "    \"Explore a new cuisine by dining at an ethnic restaurant.\",\n",
    "    \"Take a yoga class and stretch your body and mind.\",\n",
    "    \"Join a local sports league and enjoy some friendly competition.\",\n",
    "    \"Attend a workshop or lecture on a topic you're interested in.\",\n",
    "    \"Visit an amusement park and ride the roller coasters.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "doc_embeddings = model.encode(corpus_of_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query = \"What's the best activity to do with friends?\"\n",
    "query_embedding = model.encode([query])\n",
    "similarities = cosine_similarity(query_embedding, doc_embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = list(enumerate(similarities[0]))\n",
    "sorted_index = sorted(indexed, key=lambda x: x[1], reverse=True)\n",
    "print(sorted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_documents = []\n",
    "for value, score in sorted_index:\n",
    "    formatted_score = \"{:.2f}\".format(score)\n",
    "    print(f\"{formatted_score} => {corpus_of_documents[value]}\")\n",
    "    if score > 0.3:\n",
    "        recommended_documents.append(corpus_of_documents[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a bot that makes recommendations for activities. You answer in very short sentences and do not include extra information.\n",
    "These are potential activities:\n",
    "{recommended_activities}\n",
    "The user's query is: {user_input}\n",
    "Provide the user with 2 recommended activities based on their query.\n",
    "\"\"\"\n",
    "recommended_activities = \"\\n\".join(recommended_documents)\n",
    "user_input = \"I like to spend time with my friends\"\n",
    "full_prompt = prompt.format(user_input=user_input, recommended_activities=recommended_activities)\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "data = {\n",
    "    \"model\": \"mistral:latest\",\n",
    "    \"prompt\": full_prompt\n",
    "}\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "full_response=[]\n",
    "try:\n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        #filter out keep-alive new lines\n",
    "        # count += 1\n",
    "        # if count % 5== 0:\n",
    "        #     print(decoded_line['response']) # print every fifth token\n",
    "        if line:\n",
    "            decoded_line = json.loads(line.decode('utf-8'))\n",
    "            \n",
    "            full_response.append(decoded_line['response'])\n",
    "finally:\n",
    "    response.close()\n",
    "print(''.join(full_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to isolate embedding before putting into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r ../requirements.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Title, NarrativeText, Text\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "from typing import List\n",
    "import os\n",
    "import weaviate\n",
    "from weaviate.util import generate_uuid5\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "../setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "ollama.pull(\"mxbai-embed-large:v1\")\n",
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE_PATH = \"../docs/NIST.SP.800-171r2.pdf\"\n",
    "FILE_PATH = \"../docs/Player_s Handbook.pdf\"\n",
    "\n",
    "elements = partition_pdf(filename=FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [elem for elem in elements if elem.category == \"Title\"]\n",
    "\n",
    "for title in titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "narrative_texts = [elem for elem in elements if elem.category == \"NarrativeText\"]\n",
    "\n",
    "for index, elem in enumerate(narrative_texts[:5]):\n",
    "    print(f\"Narrative text {index + 1}:\")\n",
    "    print(\"\\n\".join(textwrap.wrap(elem.text, width=70)))\n",
    "    print(\"\\n\" + \"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in narrative_texts:\n",
    "    print(text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"mxbai-embed-large:v1\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pass_embedding = ollama_embedding.get_text_embedding_batch(\n",
    "    [narrative_texts[1],narrative_texts[50]], show_progress=True\n",
    ")\n",
    "print(pass_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_embedding = ollama_embedding.get_query_embedding(\"What is a paladin?\")\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_objects = []\n",
    "# Assuming you have extracted abstracts from the PDFs\n",
    "for pdf_file in list_of_pdf_files:\n",
    "    abstract = extract_abstract(pdf_file)\n",
    "    data_object = {\"source\": pdf_file.name, \"abstract\": abstract}\n",
    "    data_objects.append(data_object)\n",
    "\n",
    "# Import the objects into Weaviate\n",
    "client.batch.configure(batch_size=100)\n",
    "with client.batch as batch:\n",
    "    for data_object in data_objects:\n",
    "        batch.add_object(data_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(elements):\n",
    "  response = ollama.embeddings(model=\"mxbai-embed-large:v1\", prompt=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ollama.com/blog/embedding-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ollama chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = [\n",
    "  \"Llamas are members of the camelid family meaning they're pretty closely related to vicuñas and camels\",\n",
    "  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n",
    "  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n",
    "  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n",
    "  \"Llamas are vegetarians and have very efficient digestive systems\",\n",
    "  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\",\n",
    "]\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embeddings(model=\"mxbai-embed-large:v1\", prompt=d)\n",
    "  embedding = response[\"embedding\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=[embedding],\n",
    "    documents=[d]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example prompt\n",
    "prompt = \"What animals are llamas related to?\"\n",
    "\n",
    "# generate an embedding for the prompt and retrieve the most relevant doc\n",
    "response = ollama.embeddings(\n",
    "  prompt=prompt,\n",
    "  model=\"mxbai-embed-large:v1\"\n",
    ")\n",
    "results = collection.query(\n",
    "  query_embeddings=[response[\"embedding\"]],\n",
    "  n_results=1\n",
    ")\n",
    "data = results['documents'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a response combining the prompt and data we retrieved in step 2\n",
    "output = ollama.generate(\n",
    "  model=\"mistral\",\n",
    "  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\"\n",
    ")\n",
    "\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've proved that this actually works, let's try to use OUR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.connect_to_local()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(name=\"PHB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.create(name=\"PHB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_texts = [elem for elem in elements if elem.category == \"NarrativeText\"]\n",
    "titles = [elem for elem in elements if elem.category == \"Title\"]\n",
    "\n",
    "for index, elem in enumerate(narrative_texts[:5]):\n",
    "    response = ollama.embeddings(model=\"mxbai-embed-large:v1\", prompt=elem)\n",
    "    embedding = response[\"embedding\"]\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
